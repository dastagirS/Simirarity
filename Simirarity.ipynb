{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZT5rx1zrpow3",
        "outputId": "8185e66e-b50e-4b7c-f39b-71624ca962a9"
      },
      "outputs": [],
      "source": [
        "# IF USING GOOGLE COLAB\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_0MwlW_hgaa",
        "outputId": "81a38bcd-087b-4f53-c307-11a613c423b5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9HyTln7iBTa"
      },
      "outputs": [],
      "source": [
        "def extract_text(directory):\n",
        "    text = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith('.pdf'):\n",
        "            filepath = os.path.join(directory, filename)\n",
        "            with open(filepath, 'rb') as f:\n",
        "                pdf = PyPDF2.PdfFileReader(f)\n",
        "                for page in range(pdf.getNumPages()):\n",
        "                    text.append(pdf.getPage(page).extractText())\n",
        "    return ' '.join(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3lO719EiDH0"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    sentences = sent_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    preprocessed_sentences = []\n",
        "    for sentence in sentences:\n",
        "        words = word_tokenize(sentence.lower())\n",
        "        words = [word for word in words if word.isalpha() and word not in stop_words]\n",
        "        preprocessed_sentence = ' '.join(words)\n",
        "        if preprocessed_sentence:\n",
        "            preprocessed_sentences.append(preprocessed_sentence)\n",
        "    return preprocessed_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J48cYKgmiKBx"
      },
      "outputs": [],
      "source": [
        "def compute_tfidf_vectors(sentences):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_vectors = vectorizer.fit_transform(sentences)\n",
        "    return tfidf_vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ry_RkzzQiNVq"
      },
      "outputs": [],
      "source": [
        "def find_similar_sentences(pdf_dir, output_file, threshold=0.7, stop_words=None):\n",
        "    if not stop_words:\n",
        "        vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    sentences = []\n",
        "    for filename in os.listdir(pdf_dir):\n",
        "        if filename.endswith('.pdf'):\n",
        "            filepath = os.path.join(pdf_dir, filename)\n",
        "            with open(filepath, 'rb') as f:\n",
        "                pdf_reader = PyPDF2.PdfReader(f)\n",
        "                for page in pdf_reader.pages:\n",
        "                    text = page.extract_text()\n",
        "                    if text:\n",
        "                        for sentence in sent_tokenize(text):\n",
        "                            sentences.append(sentence)\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
        "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "    similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "    with open(output_file, 'w') as f:\n",
        "        for i in range(len(sentences)):\n",
        "            for j in range(i+1, len(sentences)):\n",
        "                if similarities[i][j] >= threshold:\n",
        "                    f.write(f'Similarity Score: {similarities[i][j]}\\n')\n",
        "                    f.write(f'Sentence 1: {sentences[i]}\\n')\n",
        "                    f.write(f'Sentence 2: {sentences[j]}\\n')\n",
        "                    f.write('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F55G8rg5ivMw"
      },
      "outputs": [],
      "source": [
        "find_similar_sentences('/myPdfs', '/myPdfs/output.txt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
